{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.models as models\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import utilities as UT\n",
    "from ranksvm import get_dynamic_image\n",
    "\n",
    "def prep_data(LABEL_PATH ,TEST_NUM):\n",
    "    # This function is used to prepare train/test labels for 5-fold cross-validation\n",
    "    TEST_LABEL = LABEL_PATH + '/fold_All_' + str(TEST_NUM) +'.csv'\n",
    "\n",
    "    # combine train labels\n",
    "    filenames = [LABEL_PATH + '/fold_All_0.csv', \n",
    "                LABEL_PATH + '/fold_All_1.csv', \n",
    "                LABEL_PATH + '/fold_All_2.csv', \n",
    "                LABEL_PATH + '/fold_All_3.csv', \n",
    "                LABEL_PATH + '/fold_All_4.csv', ]\n",
    "\n",
    "    filenames.remove(TEST_LABEL)\n",
    "\n",
    "    with open(LABEL_PATH + '/combined_train_list_All.csv', 'w') as combined_train_list:\n",
    "        for fold in filenames:\n",
    "            for line in open(fold, 'r'):                \n",
    "                combined_train_list.write(line)\n",
    "    TRAIN_LABEL = LABEL_PATH + '/combined_train_list_All.csv'\n",
    "    \n",
    "    return TRAIN_LABEL, TEST_LABEL\n",
    "    \n",
    "class Dataset_Early_Fusion(Dataset):\n",
    "    def __init__(self, \n",
    "                 label_file='/data/scratch/xxing/adni_dl/Preprocessed/ADNI2_MRItrain_list.csv'):         \n",
    "        self.files = UT.read_csv(label_file)\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    def __getitem__(self,idx):\n",
    "        temp = self.files[idx]        \n",
    "        full_path = temp[0]        \n",
    "        \n",
    "        label = full_path.split('/')[-2]\n",
    "        if(label=='CN'):\n",
    "            label=0\n",
    "        elif(label=='AD'):\n",
    "            label=1\n",
    "        elif(label=='EMCI'):\n",
    "            label=2\n",
    "        elif(label=='LMCI'):\n",
    "            label=3\n",
    "        else:\n",
    "            print('Label Error')\n",
    "        \n",
    "        im = np.load(full_path) #input image [A,S,Co,C]\n",
    "        im = np.transpose(im, (3,2,0,1)) #[C,Co,A,S]\n",
    "        #print(im.shape)\n",
    "        im = im/im.max()\n",
    "        return im, int(label), full_path # output image shape [C,W,H,T]\n",
    "\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "## Q-Net\n",
    "class Qnet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, apply_softmax=True):\n",
    "        super(Qnet, self).__init__()\n",
    "\n",
    "        self.inc = inconv(in_channels, out_channels)\n",
    "              \n",
    "        self.apply_softmax = apply_softmax\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.inc(x)\n",
    "        if self.apply_softmax:\n",
    "            x = F.softmax(x, dim = 1) # softmax across channel dimension\n",
    "        return x\n",
    "\n",
    "    \n",
    "class Self_Attn(nn.Module):\n",
    "    \"\"\" Self attention Layer\"\"\"\n",
    "    def __init__(self,in_dim):\n",
    "        super(Self_Attn,self).__init__()\n",
    "        self.chanel_in = in_dim\n",
    "        \n",
    "        \n",
    "        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8, kernel_size= 1) #original out=in//8\n",
    "        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8, kernel_size= 1)  #original out=in//8\n",
    "        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax  = nn.Softmax(dim=-1) \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "            inputs :\n",
    "                x : input feature maps( B X C X W X H)\n",
    "            returns :\n",
    "                out : self attention value + input feature \n",
    "                attention: B X N X N (N is Width*Height)\n",
    "        \"\"\"\n",
    "        m_batchsize,C,width ,height = x.size()\n",
    "        proj_query  = self.query_conv(x).view(m_batchsize,-1,width*height).permute(0,2,1) # B X C X (N)\n",
    "        proj_key =  self.key_conv(x).view(m_batchsize,-1,width*height) # B X C x (*W*H)\n",
    "        energy =  torch.bmm(proj_query,proj_key) # transpose check\n",
    "        attention = self.softmax(energy) # BX (N) X (N) \n",
    "        proj_value = self.value_conv(x).view(m_batchsize,-1,width*height) # B X C X N\n",
    "\n",
    "        out = torch.bmm(proj_value,attention.permute(0,2,1) )\n",
    "        out = out.view(m_batchsize,C,width,height)\n",
    "        \n",
    "        out = self.gamma*out + x\n",
    "        return out#,attention\n",
    "\n",
    "class eca_layer(nn.Module):\n",
    "    \"\"\"Constructs a ECA module.\n",
    "    Args:\n",
    "        channel: Number of channels of the input feature map\n",
    "        k_size: Adaptive selection of kernel size\n",
    "    \"\"\"\n",
    "    def __init__(self, channel):\n",
    "        super(eca_layer, self).__init__()\n",
    "        k_size=5\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False) \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: input features with shape [b, c, h, w]\n",
    "        b, c, h, w = x.size()\n",
    "\n",
    "        # feature descriptor on the global spatial information\n",
    "        y = self.avg_pool(x)\n",
    "\n",
    "        # Two different branches of ECA module\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "\n",
    "        # Multi-scale information fusion\n",
    "        y = self.sigmoid(y)\n",
    "\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_classes=4, \n",
    "                 feature='Vgg11', \n",
    "                 pretrained=True, \n",
    "                 requires_grad=True):         \n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "         # Feature Extraction\n",
    "        if(feature=='Alex'):\n",
    "            self.ft_ext = models.alexnet(pretrained=pretrained) \n",
    "            self.ft_ext_modules = list(self.ft_ext.children())[:-2]            \n",
    "            \n",
    "        elif(feature=='Res34'):\n",
    "            self.ft_ext = models.resnet34(pretrained=pretrained) \n",
    "            self.ft_ext_modules=list(self.ft_ext.children())[0:-2] # remove the Maxpooling layer\n",
    "            \n",
    "        elif(feature=='Res101'):\n",
    "            self.ft_ext = models.resnet101(pretrained=pretrained) \n",
    "            self.ft_ext_modules=list(self.ft_ext.children())[0:-2] # remove the Maxpooling layer\n",
    "            \n",
    "        elif(feature=='Vgg16'):\n",
    "            self.ft_ext = models.vgg16(pretrained=pretrained) \n",
    "            self.ft_ext_modules=list(self.ft_ext.children())[0] # remove the Maxpooling layer\n",
    "            \n",
    "        elif(feature=='Vgg11'):\n",
    "            self.ft_ext = models.vgg11(pretrained=pretrained) \n",
    "            self.ft_ext_modules=list(self.ft_ext.children())[0] # remove the Maxpooling layer\n",
    "            \n",
    "        elif(feature=='Mobile'):\n",
    "            self.ft_ext = models.mobilenet_v2(pretrained=pretrained) \n",
    "            self.ft_ext_modules=list(self.ft_ext.children())[0] # remove the Maxpooling layer\n",
    "            \n",
    "        self.ft_ext=nn.Sequential(*self.ft_ext_modules)                \n",
    "        for p in self.ft_ext.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "            \n",
    "        # Classifier\n",
    "        if(feature=='Alex'):\n",
    "            feature_shape=(256,5,5)\n",
    "        elif(feature=='Res34'):\n",
    "            feature_shape=(512,3,5)\n",
    "        elif(feature=='Res101'):\n",
    "            feature_shape=(512,5,5)\n",
    "        elif(feature=='Vgg16'):\n",
    "            feature_shape=(512,6,6)\n",
    "        elif(feature=='Vgg11'):\n",
    "            feature_shape=(512,6,6)\n",
    "        elif(feature=='Mobile'):\n",
    "            feature_shape=(1280,4,4)\n",
    "            \n",
    "        conv1_output_features = int(feature_shape[0])\n",
    "        \n",
    "        fc1_input_features = int(conv1_output_features*feature_shape[1]*feature_shape[2])\n",
    "        fc1_output_features = int(conv1_output_features*2)\n",
    "        fc2_output_features = int(fc1_output_features/4)\n",
    "        \n",
    "        self.sattn=Self_Attn(conv1_output_features)\n",
    "        self.eca = eca_layer(conv1_output_features)\n",
    "                \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=feature_shape[0],      \n",
    "                out_channels=conv1_output_features,    \n",
    "                kernel_size=1,       \n",
    "            ),\n",
    "            nn.BatchNorm2d(conv1_output_features),\n",
    "            nn.ReLU()\n",
    "        )                    \n",
    "        self.fc1 = nn.Sequential(\n",
    "             nn.Linear(fc1_input_features, fc1_output_features),\n",
    "             nn.BatchNorm1d(fc1_output_features),            \n",
    "             nn.ReLU()\n",
    "         )\n",
    "\n",
    "        self.fc2 = nn.Sequential(\n",
    "             nn.Linear(fc1_output_features, fc2_output_features),\n",
    "             nn.BatchNorm1d(fc2_output_features),\n",
    "             nn.ReLU()\n",
    "         )\n",
    "        \n",
    "        self.out = nn.Linear(fc2_output_features, num_classes)\n",
    "        \n",
    "    def forward(self, x, drop_prob=0.5):\n",
    "        x = self.ft_ext(x)\n",
    "        xa = self.sattn(x)\n",
    "        xc = self.eca(x)\n",
    "        x = xa + xc\n",
    "        #x = xc\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.fc1(x)\n",
    "        x = nn.Dropout(drop_prob)(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.Dropout(drop_prob)(x)        \n",
    "        prob = self.out(x) \n",
    "        \n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainexp(train_dataloader, val_dataloader, feature='Res34', batch_size=16):\n",
    "    \n",
    "    net = CNN(feature=feature).to(device)\n",
    "    q_net = Qnet(in_channels=1, out_channels=1, apply_softmax=False).to(device)\n",
    "    \n",
    "    param = list(net.parameters()) + list(q_net.parameters())\n",
    "    opt = torch.optim.Adam(param, lr=0.0001, weight_decay=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(opt, gamma= 0.985)\n",
    "\n",
    "    loss_fcn = torch.nn.CrossEntropyLoss(weight=LOSS_WEIGHTS.to(device))\n",
    "        \n",
    "    t = trange(EPOCHS, desc=' ', leave=True)\n",
    "\n",
    "    train_hist = []\n",
    "    val_hist = []\n",
    "    \n",
    "    pred_result = []\n",
    "    old_acc = 0\n",
    "    old_auc = 0\n",
    "    test_acc = 0\n",
    "    best_epoch = 0\n",
    "    for e in t:    \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        \n",
    "        val_y_true = []\n",
    "        val_y_pred = []                \n",
    "        \n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        \n",
    "        val_score =[]\n",
    "        val_score_f =[]\n",
    "        val_img = []\n",
    "        val_label = []\n",
    "        val_fuse = []\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        # training\n",
    "        net.train()\n",
    "        q_net.train()\n",
    "        \n",
    "        for step, (img, label, _) in enumerate(train_dataloader):\n",
    "            img = img.float().to(device)\n",
    "            \n",
    "            label = label.long().to(device)\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # initialize the quality score tensor\n",
    "            quality_score = torch.zeros(img.shape[0], 160, 1,1).to(device)\n",
    "            # Compute logits of quality scores\n",
    "            for i in range(img.shape[2]):  # for every slice\n",
    "                slice_i = img[:, :, i, :, :]\n",
    "                quality_score[:, i, :, :] = q_net(slice_i).squeeze(1)\n",
    "  \n",
    "            # Normalize quality scores: apply Softmax across slices dimension. We are using slices in dim=1.\n",
    "            quality_final = torch.softmax(quality_score, dim=1)\n",
    "            #print(quality_final)\n",
    "            # initialize the fused image\n",
    "            fused_image = torch.zeros(img.shape[0], 1, 96, 160).to(device)\n",
    "            #print('initializing fused image of shape: ', fused_image.shape)\n",
    "\n",
    "            # Compute the fused image\n",
    "            for i in range(img.shape[2]):  # for every slice\n",
    "                slice_i = img[:, :, i, :, :]\n",
    "                fused_image += quality_final[:, i, :, :].unsqueeze(1) * slice_i\n",
    "                \n",
    "            fused_image = torch.cat((fused_image,fused_image,fused_image), dim=1)\n",
    "            #print('initializing fused image of shape: ', fused_image.shape)\n",
    "            \n",
    "            out = net(fused_image)\n",
    "            loss = loss_fcn(out, label)\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            label = label.cpu().detach()\n",
    "            out = out.cpu().detach()\n",
    "            y_true, y_pred = UT.assemble_labels(step, y_true, y_pred, label, out)        \n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss/(step+1)\n",
    "        \n",
    "        acc = float(torch.sum(torch.max(y_pred, 1)[1]==y_true))/ float(len(y_pred))\n",
    "        #auc = metrics.roc_auc_score(y_true, y_pred[:,1])\n",
    "        f1 = metrics.f1_score(y_true, torch.max(y_pred, 1)[1], average='micro')\n",
    "        precision = metrics.precision_score(y_true, torch.max(y_pred, 1)[1], average='micro')\n",
    "        recall = metrics.recall_score(y_true, torch.max(y_pred, 1)[1], average='micro')\n",
    "        #ap = metrics.average_precision_score(y_true, torch.max(y_pred, 1)[1], average='micro') #average_precision\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # val\n",
    "        net.eval()\n",
    "        q_net.eval()\n",
    "        full_path = []\n",
    "        with torch.no_grad():\n",
    "            for step, (img, label, _) in enumerate(val_dataloader):\n",
    "                \n",
    "                \n",
    "                val_img.append(img)\n",
    "                val_label.append(label)\n",
    "                \n",
    "                img = img.float().to(device)\n",
    "                label = label.long().to(device)\n",
    "                \n",
    "                # initialize the quality score tensor\n",
    "                quality_score = torch.zeros(img.shape[0], 160, 1, 1).to(device)\n",
    "                # Compute logits of quality scores\n",
    "                for i in range(img.shape[2]):  # for every slice\n",
    "                    slice_i = img[:, :, i, :, :]\n",
    "                    quality_score[:, i, :, :] = q_net(slice_i).squeeze(1)\n",
    "  \n",
    "                # Normalize quality scores: apply Softmax across slices dimension. We are using slices in dim=1.\n",
    "                quality_final = torch.softmax(quality_score, dim=1)\n",
    "        \n",
    "                val_score.append(quality_score.cpu())\n",
    "                val_score_f.append(quality_final.cpu())\n",
    "                # initialize the fused image\n",
    "                fused_image = torch.zeros(img.shape[0], 1, 96, 160).to(device)\n",
    "                #print('initializing fused image of shape: ', fused_image.shape)\n",
    "                \n",
    "                # Compute the fused image\n",
    "                for i in range(img.shape[2]):  # for every slice\n",
    "                    slice_i = img[:, :, i, :, :]\n",
    "                    fused_image += quality_final[:, i, :, :].unsqueeze(1) * slice_i\n",
    "                \n",
    "                fused_image = torch.cat((fused_image,fused_image,fused_image), dim=1)\n",
    "                val_fuse.append(fused_image.cpu())\n",
    "                \n",
    "                out = net(fused_image)\n",
    "                loss = loss_fcn(out, label)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                label = label.cpu().detach()\n",
    "                out = out.cpu().detach()\n",
    "                val_y_true, val_y_pred = UT.assemble_labels(step, val_y_true, val_y_pred, label, out)\n",
    "                \n",
    "                for item in _:\n",
    "                    full_path.append(item)\n",
    "                \n",
    "        val_loss = val_loss/(step+1)\n",
    "        #print(val_y_pred)\n",
    "        \n",
    "        val_acc = float(torch.sum(torch.max(val_y_pred, 1)[1]==val_y_true))/ float(len(val_y_pred))\n",
    "        #val_auc = metrics.roc_auc_score(val_y_true, val_y_pred[:,1])\n",
    "        val_f1 = metrics.f1_score(val_y_true, torch.max(val_y_pred, 1)[1], average='micro')\n",
    "        val_precision = metrics.precision_score(val_y_true, torch.max(val_y_pred, 1)[1], average='micro')\n",
    "        val_recall = metrics.recall_score(val_y_true, torch.max(val_y_pred, 1)[1], average='micro')\n",
    "        #val_ap = metrics.average_precision_score(val_y_true, torch.max(val_y_pred, 1)[1]) #average_precision\n",
    "\n",
    "\n",
    "        train_hist.append([train_loss, acc, f1, precision, recall])\n",
    "        val_hist.append([val_loss, val_acc,  val_f1, val_precision, val_recall])             \n",
    "\n",
    "        t.set_description(\"Epoch: %i, train loss: %.4f, train acc: %.4f, val loss: %.4f, val acc: %.4f, test acc: %.4f\" \n",
    "                          %(e, train_loss, acc, val_loss, val_acc, test_acc))\n",
    "\n",
    "\n",
    "        if(old_acc<val_acc):\n",
    "            old_acc = val_acc\n",
    "            best_epoch = e\n",
    "            test_loss = 0\n",
    "            test_y_true = val_y_true\n",
    "            test_y_pred = val_y_pred            \n",
    "\n",
    "            test_loss = val_loss\n",
    "            test_acc = float(torch.sum(torch.max(test_y_pred, 1)[1]==test_y_true))/ float(len(test_y_pred))\n",
    "            #test_auc = metrics.roc_auc_score(test_y_true, test_y_pred[:,1])\n",
    "            test_f1 = metrics.f1_score(test_y_true, torch.max(test_y_pred, 1)[1], average='micro')\n",
    "            test_precision = metrics.precision_score(test_y_true, torch.max(test_y_pred, 1)[1], average='micro')\n",
    "            test_recall = metrics.recall_score(test_y_true, torch.max(test_y_pred, 1)[1], average='micro')\n",
    "            #test_ap = metrics.average_precision_score(test_y_true, torch.max(test_y_pred, 1)[1], average='micro') #average_precision\n",
    "            \n",
    "            test_performance = [best_epoch, test_loss, test_acc, test_f1, test_precision, test_recall]\n",
    "            val_score_b= val_score\n",
    "            val_fuse_b = val_fuse\n",
    "            val_score_fb= val_score_f\n",
    "\n",
    "    return train_hist, val_hist, test_performance, test_y_true, test_y_pred, full_path, val_img, val_label, val_score_b, val_fuse_b,val_score_fb\n",
    "    \n",
    "LABEL_PATH = '/u/amo-d0/grad/xxi242/Preprocessed/ADNI_AV45_AF'\n",
    "\n",
    "GPU = 1\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 150\n",
    "\n",
    "LR = 0.0001\n",
    "#LOSS_WEIGHTS = torch.tensor([1., 1.28, 1, 1.29 ]) \n",
    "\n",
    "device = torch.device('cuda:'+str(GPU) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_hist = []\n",
    "val_hist = []\n",
    "test_performance = []\n",
    "test_y_true = np.asarray([])\n",
    "test_y_pred = np.asarray([])\n",
    "full_path = np.asarray([])\n",
    "for i in range(0, 5):\n",
    "    print('Train Fold', i)\n",
    "    \n",
    "    TEST_NUM = i\n",
    "    TRAIN_LABEL, TEST_LABEL = prep_data(LABEL_PATH, TEST_NUM)\n",
    "    \n",
    "    train_dataset = Dataset_Early_Fusion(label_file=TRAIN_LABEL)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, num_workers=1, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "    val_dataset = Dataset_Early_Fusion(label_file=TEST_LABEL)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, num_workers=1, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "        \n",
    "    cur_result = trainexp(train_dataloader, val_dataloader, batch_size=16)\n",
    "    \n",
    "    train_hist.append(cur_result[0])\n",
    "    val_hist.append(cur_result[1]) \n",
    "    test_performance.append(cur_result[2]) \n",
    "    test_y_true = np.concatenate((test_y_true, cur_result[3].numpy()))\n",
    "    if(len(test_y_pred) == 0):\n",
    "        test_y_pred = cur_result[4].numpy()\n",
    "    else:\n",
    "        test_y_pred = np.vstack((test_y_pred, cur_result[4].numpy()))\n",
    "    full_path = np.concatenate((full_path, np.asarray(cur_result[5])))\n",
    "\n",
    "print(test_performance)\n",
    "\n",
    "test_y_true = torch.tensor(test_y_true)\n",
    "test_y_pred = torch.tensor(test_y_pred)\n",
    "test_acc = float(torch.sum(torch.max(test_y_pred, 1)[1]==test_y_true.long()))/ float(len(test_y_pred))\n",
    "#test_auc = metrics.roc_auc_score(test_y_true, test_y_pred[:,1])\n",
    "test_f1 = metrics.f1_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "test_precision = metrics.precision_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "test_recall = metrics.recall_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "#test_ap = metrics.average_precision_score(test_y_true, torch.max(test_y_pred, 1)[1])\n",
    "\n",
    "print('ACC %.4f, F1 %.4f, Prec %.4f, Recall %.4f' \n",
    "      %(test_acc, test_f1, test_precision, test_recall))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
